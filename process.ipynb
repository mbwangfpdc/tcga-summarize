{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.6.post1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 18:23:22 config.py:510] This model supports multiple tasks: {'embed', 'score', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 01-11 18:23:22 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 01-11 18:23:22 config.py:642] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 01-11 18:23:22 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='BioMistral/BioMistral-7B', speculative_config=None, tokenizer='BioMistral/BioMistral-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=BioMistral/BioMistral-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 01-11 18:23:33 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-11 18:23:48 model_runner.py:1094] Starting to load model BioMistral/BioMistral-7B...\n",
      "INFO 01-11 18:23:48 weight_utils.py:251] Using model weights format ['*.bin']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f333b386f074427b2a3a8b03a9cd469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-9morg0/miniconda3/envs/process-reports/lib/python3.10/site-packages/vllm/model_executor/model_loader/weight_utils.py:450: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-11 18:23:57 model_runner.py:1099] Loading model weights took 13.4966 GB\n",
      "INFO 01-11 18:23:59 worker.py:241] Memory profiling takes 2.57 seconds\n",
      "INFO 01-11 18:23:59 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB\n",
      "INFO 01-11 18:23:59 worker.py:241] model weights take 13.50GiB; non_torch_memory takes 0.10GiB; PyTorch activation peak memory takes 3.38GiB; the rest of the memory reserved for KV Cache is 54.26GiB.\n",
      "INFO 01-11 18:23:59 gpu_executor.py:76] # GPU blocks: 27780, # CPU blocks: 2048\n",
      "INFO 01-11 18:23:59 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 13.56x\n",
      "INFO 01-11 18:24:01 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 4.49 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "token = \"CENSORED\"\n",
    "model = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "model = \"BioMistral/BioMistral-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "llm = LLM(model=model, enforce_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a reports csv and return it as a list of pairs\n",
    "import csv\n",
    "def reports_from_file(filename) -> list[tuple[str, str]]:\n",
    "    data = []\n",
    "    with open(filename, newline=\"\") as reports:\n",
    "        reader = csv.DictReader(reports)\n",
    "        for row in reader:\n",
    "            data.append((row[\"case_id\"], row[\"text\"]))\n",
    "    return data\n",
    "def reports_to_file(filename, data: list[tuple[str, str]]):\n",
    "    with open(filename, \"w+\", newline=\"\") as reports:\n",
    "        writer = csv.DictWriter(reports, fieldnames=[\"case_id\", \"text\"])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow({\"case_id\": row[0], \"text\": row[1]})\n",
    "reports = reports_from_file(\"TCGA_Reports_Processed.csv\")\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(reports)\n",
    "def write_for_compare(input_str: str, transformed: str):\n",
    "    with open(\"raw_text.txt\", \"a+\") as base_out:\n",
    "        base_out.write(input_str + \"\\n\")\n",
    "        base_out.write(\"======END OF PROMPT======\\n\")\n",
    "    with open(\"summarized.txt\", \"a+\") as test_out:\n",
    "        test_out.write(transformed + \"\\n\")\n",
    "        test_out.write(\"======END OF OUTPUT======\\n\")\n",
    "\n",
    "def reset_compare():\n",
    "    try:\n",
    "        import os\n",
    "        os.remove(\"raw_text.txt\", )\n",
    "        os.remove(\"summarized.txt\")\n",
    "    except OSError:\n",
    "        return\n",
    "\n",
    "PROMPT = \"Please repeat the diagnosis, clinical history, and clinician comments from this report without rewording it or adding additional text:\"\n",
    "# PROMPT = \"Please remove the 'gross description' from this report, if it is present:\"\n",
    "PROMPT = \"Extract and repeat the results of the following clinical report in a single paragraph. Focus on test results, diagnoses and clinical history. Omit the gross description. Do not acknowledge this prompt.\"\n",
    "ACK = \"Here is the report summary:\"\n",
    "\n",
    "\n",
    "def make_prompt(report: str) -> str:\n",
    "    return f\"{PROMPT}\\nINPUT:\\n{report}\\nINPUT_DONE\\nOUTPUT:\\n\"\n",
    "def make_example(example_report: str, fixed_report: str) -> str:\n",
    "    return f\"{make_prompt(example_report)}{fixed_report}\\nOUTPUT_DONE\\n===\\n\"\n",
    "def make_prompt_for_chat(case: str, report: str) -> str:\n",
    "    return f\"\\n{PROMPT}\\n{case} - {report}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EX = 10\n",
    "reports_to_file(\"examples.csv\", reports[-NUM_TRAIN_EX:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:06<00:00,  2.92it/s, est. speed input: 3529.65 toks/s, output: 422.55 toks/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "examples = reports_from_file(\"examples.csv\")\n",
    "examples_fixed = reports_from_file(\"examples_fixed.csv\")\n",
    "assert(len(examples) == len(examples_fixed))\n",
    "report_hints = [(case, e, ef) for (case, e), (_, ef) in zip(examples, examples_fixed)]\n",
    "\n",
    "TRAIN_NUM = 3\n",
    "random.shuffle(report_hints)\n",
    "report_hints = report_hints[:TRAIN_NUM]\n",
    "\n",
    "\n",
    "# Make the full prompt including the preprompt\n",
    "# def make_full_prompt(report: str) -> str:\n",
    "#     return preprompt + make_prompt(report)\n",
    "# TYPOS = \"fix typos\"\n",
    "# DIAGNOSTIC = \"exclude sections like method notes and gross description\"\n",
    "# CONCISE = \"make sentences more concise\"\n",
    "# preprompt = f\"Summarize this pathological report, but first {', '.join([DIAGNOSTIC])}: \\n\\n\"\n",
    "# preprompt = \"Extract diagnostic sections from this report without rewording it or adding additional text:\\n\"\n",
    "chat_preprompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Please clean up the following text medical reports. Please do not modify or add to the reports unless instructed otherwise.\"\n",
    "    },\n",
    "]\n",
    "for case, ex_in, ex_out in report_hints:\n",
    "    chat_preprompt.append({\"role\": \"user\", \"content\": make_prompt_for_chat(case, ex_in)})\n",
    "    chat_preprompt.append({\"role\": \"assistant\", \"content\": f\"{ACK}\\n{case} - {ex_out}\"})\n",
    "# Mistral models don't do well with the chat history\n",
    "chat_preprompt = []\n",
    "\n",
    "testonly = True\n",
    "NUM = 20 if testonly else len(reports)\n",
    "convos = [copy.deepcopy(chat_preprompt) for _ in range(NUM)]\n",
    "print(convos)\n",
    "if testonly:\n",
    "    # For tests, shuffle the reports so we can do a diversity of testing\n",
    "    myreports = copy.deepcopy(reports)\n",
    "    random.shuffle(myreports)\n",
    "else:\n",
    "    myreports = reports\n",
    "for convo, (case, report) in zip(convos, myreports):\n",
    "    convo.append({\"role\": \"user\", \"content\": make_prompt_for_chat(case, report)})\n",
    "sampling_params = SamplingParams(max_tokens=1000, temperature=0, repetition_penalty=1.1)\n",
    "outputs = llm.chat(messages=convos, sampling_params=sampling_params)\n",
    "# outputs = []\n",
    "# for convo in convos:\n",
    "#     outputs.extend(llm.chat(conversations, sampling_params=sampling_params))\n",
    "\n",
    "if testonly:\n",
    "    reset_compare()\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        write_for_compare(prompt, generated_text)\n",
    "        # print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "else:\n",
    "    new_reports = []\n",
    "    for (case, _), output in outputs:\n",
    "        generated_text = output.outputs[0].text\n",
    "        generated_text = generated_text.replace(\"\\n\", \" \")\n",
    "        new_reports.append((case, generated_text))\n",
    "    reports_to_file(\"TCGA_Reports_Transformed.csv\", new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reports = []\n",
    "for (case, _), output in zip(myreports, outputs):\n",
    "    generated_text = output.outputs[0].text\n",
    "    generated_text = generated_text.replace(\"\\n\", \" \")\n",
    "    new_reports.append((case, generated_text))\n",
    "reports_to_file(\"TCGA_Reports_Transformed.csv\", new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_transformed = reports_from_file(\"TCGA_Reports_Transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process-reports",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
